{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Effective Agents (with Pydantic AI)\n",
    "\n",
    "Examples for the agentic workflows discussed in\n",
    "[Building Effective Agents](https://www.anthropic.com/research/building-effective-agents)\n",
    "by [Erik Schluntz](https://github.com/eschluntz) and [Barry Zhang](https://github.com/ItsBarryZ)\n",
    "of Anthropic, inspired, ported and adapted from the\n",
    "[code samples](https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents)\n",
    "by the authors using [Pydantic AI](https://ai.pydantic.dev/).\n",
    "\n",
    "## Evaluator - Optimizer\n",
    "Examples copied from [Intellectronica - Building Effective Agents with Pydantic AI](https://github.com/intellectronica/building-effective-agents-with-pydantic-ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "from IPython.display import clear_output ; clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available AI models:\n",
      "['openai:gpt-4o',\n",
      " 'openai:gpt-4o-mini',\n",
      " 'ollama:deepseek-coder:6.7b',\n",
      " 'ollama:llama3.2:3b',\n",
      " 'ollama:mistral:latest',\n",
      " 'ollama:phi4-mini:latest',\n",
      " 'ollama:gemma3:latest',\n",
      " 'ollama:phi4-mini-reasoning:latest',\n",
      " 'ollama:llama3.2:latest',\n",
      " 'ollama:nomic-embed-text:latest',\n",
      " 'ollama:llama3.1:8b',\n",
      " 'ollama:phi3.5:latest']\n",
      "\n",
      "Using AI model: ollama:phi3.5:latest\n",
      "Configuring Ollama model: phi3.5:latest at http://localhost:11434/v1\n"
     ]
    }
   ],
   "source": [
    "from util import initialize, show\n",
    "AI_MODEL = initialize()\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow: Evaluator - Optimizer\n",
    "\n",
    "While executing a single call to an LLM with a good prompt and sufficient context\n",
    "often yields satisfactory results, the first run isn't always the best we can\n",
    "achieve. By iteratively getting the LLM to generate a result, and then evaluate\n",
    "the result and propose improvements, we can achieve much higher quality.\n",
    "\n",
    "> <img src=\"https://ai.pydantic.dev/img/pydantic-ai-dark.svg\" style=\"height: 1em;\" />\n",
    "> The schema definition derived from the Pydantic models we define is primarily\n",
    "> used to control the result we read from the LLM call, but in many cases it\n",
    "> is also possible to use it to instruct the LLM on the desired behaviour.\n",
    "> Here, for example, we use a `thoughts` field to get the LLM to engage in\n",
    "> \"chain-of-thought\" generation, which helps it in reasoning. By generating the\n",
    "> content of this field, the LLM directs itself towards a more detailed and precise\n",
    "> response. Even if we don't need to read the value generated, we can still inspect\n",
    "> it in debugging or using an observability tool like Pydantic Logfire to understand\n",
    "> how the LLM approaches the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorResponse(BaseModel):\n",
    "    thoughts: str = Field(..., description=(\n",
    "        'Your understanding of the task and feedback '\n",
    "        'and how you plan to improve.'\n",
    "    ))\n",
    "    response: str = Field(..., description='The generated solution.')\n",
    "\n",
    "\n",
    "async def generate(prompt: str, task: str, context: str = \"\") -> tuple[str, str]:\n",
    "    \"\"\"Generate and improve a solution based on feedback.\"\"\"\n",
    "    system_prompt = prompt\n",
    "    if context:\n",
    "        system_prompt += f\"\\n\\n{context}\"\n",
    "\n",
    "    generator_agent = Agent(\n",
    "        AI_MODEL,\n",
    "        system_prompt=system_prompt,\n",
    "        output_type=GeneratorResponse,\n",
    "    )\n",
    "    response = await generator_agent.run(f'Task:\\n{task}')\n",
    "\n",
    "    thoughts = response.output.thoughts\n",
    "    result = response.output.response\n",
    "    \n",
    "    show('', title='Generation')\n",
    "    show(thoughts, title='Thoughts')\n",
    "    show(result, title='Generated')\n",
    "    \n",
    "    return thoughts, result\n",
    "\n",
    "\n",
    "class EvaluatorResponse(BaseModel):\n",
    "    thoughts: str = Field(..., description=(\n",
    "        'Your careful and detailed review and evaluation of the submited content.'\n",
    "    ))\n",
    "    evaluation: str = Field(..., description='PASS, NEEDS_IMPROVEMENT, or FAIL')\n",
    "    feedback: str = Field(..., description='What needs improvement and why.')\n",
    "\n",
    "\n",
    "async def evaluate(prompt: str, content: str, task: str) -> tuple[str, str]:\n",
    "    \"\"\"Evaluate if a solution meets requirements.\"\"\"\n",
    "    evaluator_agent = Agent(\n",
    "        AI_MODEL,\n",
    "        system_prompt=f'{prompt}\\n\\nTask:\\n{task}',\n",
    "        output_type=EvaluatorResponse,\n",
    "    )\n",
    "    response = await evaluator_agent.run(content)\n",
    "    evaluation = response.output.evaluation\n",
    "    feedback = response.output.feedback\n",
    "    \n",
    "    show('', title='Evaluation')\n",
    "    show(evaluation, title='Status')\n",
    "    show(feedback, title='Feedback')\n",
    "    \n",
    "    return evaluation, feedback\n",
    "\n",
    "\n",
    "async def loop(\n",
    "        task: str, evaluator_prompt: str, generator_prompt: str\n",
    "    ) -> tuple[str, list[dict]]:\n",
    "    \"\"\"Keep generating and evaluating until requirements are met.\"\"\"\n",
    "    memory = []\n",
    "    chain_of_thought = []\n",
    "    \n",
    "    thoughts, result = await generate(generator_prompt, task)\n",
    "    memory.append(result)\n",
    "    chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})\n",
    "    \n",
    "    while True:\n",
    "        evaluation, feedback = await evaluate(evaluator_prompt, result, task)\n",
    "        if evaluation == \"PASS\":\n",
    "            return result, chain_of_thought\n",
    "            \n",
    "        context = \"\\n\".join([\n",
    "            \"Previous attempts:\",\n",
    "            *[f\"- {m}\" for m in memory],\n",
    "            f\"\\nFeedback: {feedback}\"\n",
    "        ])\n",
    "        \n",
    "        thoughts, result = await generate(generator_prompt, task, context)\n",
    "        memory.append(result)\n",
    "        chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_prompt = \"\"\"\n",
    "Evaluate this following code implementation for:\n",
    "1. Code correctness: does it implement what is required in the spec flawlessly?\n",
    "2. Time complexity: does the implementation meet the time complexity requirements?\n",
    "3. Efficiency: is the implementation the most efficient and optimized possible for the requirements?\n",
    "4. Style and best practices: does the code follow standard Python style and best practices?\n",
    "5. Readability: is the code easy to read and understand?\n",
    "6. Documentation: is the code clearly documented, with docstrings for all functions and classes, and with inline comments where necessary?\n",
    "\n",
    "You should be evaluating only and not attemping to solve the task.\n",
    "Evaluate the code carefully and critically and make sure you don't\n",
    "miss any opportunities for improvement.\n",
    "Only output \"PASS\" if all the evaluation criteria are met and you\n",
    "have no further suggestions for improvements, otherwise output\n",
    "\"NEEDS_IMPROVEMENT\" or \"FAIL\" so that the coder can learn and improve.\n",
    "\"\"\"\n",
    "\n",
    "generator_prompt = \"\"\"\n",
    "Your goal is to complete the task based on the user input. If there are feedback \n",
    "from your previous generations, you should reflect on them to improve your solution.\"\"\"\n",
    "\n",
    "task = \"\"\"\n",
    "Implement a Stack with:\n",
    "1. push(x)\n",
    "2. pop()\n",
    "3. getMin()\n",
    "All operations should be O(1).\n",
    "\"\"\"\n",
    "\n",
    "result, chain_of_thought = await loop(task, evaluator_prompt, generator_prompt)\n",
    "\n",
    "show(result, title='Final Result')\n",
    "show(chain_of_thought, title='Chain of Thought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
